### Ковариация (covariation-совместное изм.)
$$m_{1,1}^0=cov(r_1,r_2)=\sum_{i=1}^n\sum_{j=1}^m(x_i-M[r_1])(y_j-M[r_2])q_{ij}$$
Пусть $r_1,r_2$ — независимые, тогда $$cov(r_1,r_2)=\sum_{i=1}^n\sum_{j=1}^m(x_i-M[r_1])(y_j-M[r_2])q_{ij}$$а если $r_1,r_2$ независимые, то $$q_{ij}=p_{1i}\cdot p_{2j}=p(r_1=x_i)\cdot p(r_2=y_j)$$Получается, что
$$cov(r_1,r_2)=\sum_{i=1}^n(x_i-M[r_1])p_{1i}\cdot\underbrace{\sum_{j=1}^m(y_j-M[r_2])p_{2j}}_{m_1^0[r_2]=0}=0 \Rightarrow$$$\Rightarrow cov$ — это индикатор независимости.
Обратное, вообще говоря, неверно:
$$cov(r_1,r_2)=\sum_{i=1}^n\sum_{j=1}^m(x_i-M[r_1])(y_j-M[r_2])q_{ij}=0 \Rightarrow q_{ij} = p_{1i}p_{2j}$$Что значит, если $cov(v_1,v_2)=5$?
Неизвестно, т.к. зависимость есть, но её сила и природа непонятна.
Какова область возможных значений у $cov$?
### Неравенство Коши–Буняковского
Неравенство Коши–Буняковского для заданной меры $q_{i, j}(q_{ij}\ge0 \forall i, j, \sum_{i=1}^n\sum_{j=1}^n q_{ij}=1)$
$$\left|\sum_{i=1}^n\sum_{j=1}^m f(x_i)g(y_j)q_{ij}\right| \le \sqrt{\sum_{i=1}^n\sum_{j=1}^m f^2(x_i)q_{ij}} \cdot \sqrt{\sum_{i=1}^n\sum_{j=1}^m g^2(y_j)q_{ij}}$$
$f$ и $g$ — произвольные.
В нашем случае
$$
\begin{aligned}
& f(x)=x-M[r_1] \\
& g(y)=~y-M[r_2]
\end{aligned}
~{\Longrightarrow}~
|cov(r_1,r_2)| \le \sqrt{\underbrace{(\sum_{i=1}^n\sum_{j=1}^m (x_i-M[r_1])^2q_{ij})}_{D[r_1]}\cdot\underbrace{(\sum_{i=1}^n\sum_{j=1}^m (y_j-M[r_2])^2q_{ij})}_{D[r_2]}}=\\
$$
$$=\sqrt{D[r_1]\cdot D[r_2]}$$
Вывод: 
- если $D(r_1)=D(r_2)=0,01$, то $cov(r_1,r_2)=0,01$ — говорит о сильной связи.
- если $D(x_1x_2)=D(x_1)=100$, то $cov(r_1, r_2) = 10$ — это почти то же самое, что $cov(x_1,x_2)\simeq 0$.

Чтобы получить понятную шкалу, нужна нормировка:
$$\boxed{\rho(r_1,r_2)=\frac{cov(r_1,r_2)}{\sqrt{D(r_1)D(r_2)}}}$$
$$-1\le p(x_1,x_2)\le 1$$
коэффициент корреляции (correlation = совместная взаимосвязь), это безразмерная величина (нормированное значение ковариации).
### Свойства
Свойства:
1. $|p(r_1,r_2)|\le1$
2. если $r_1$ и $r_2$ независимы, то $p(r_1,r_2)=0$
   (но обратное неверно — если $p(r_1,r_2)=0$, это не значит, что они независимы).
   Это означает, что нет корреляции, т.е. $p(r_1,r_2)=0$ - все выглядит как независимость, но утверждать наверняка не можем.
3. $\rho (a\cdot r_1 + b,~c\cdot r_2 + d) = ?$
$$D[a\cdot r_1 + b] = a^2\cdot D[r_1]$$
$$D[c\cdot r_2 + d] = c^2\cdot D[r_2]$$
$$cov(a\cdot r_1 + b,~c\cdot r_2 + d) = \sum_{i=1}^n \sum_{j=1}^m (ax_1 + b - \underbrace{M[ar_1+b]}_{aM[r_1]+b})\cdot (cy_j + d - \underbrace{M[cr_2+d]}_{cM[r_2]+d})\cdot q_{ij} =$$
$$=ac\sum_{i=1}^{n}\sum_{j=1}^{m} (x_i-M[r_1])(y_j-M[r_2])q_{ij}=ac\cdot cov(r_1,r_2)$$
$$\rho(a \cdot r_1 + b,c \cdot r_2 + d)=\frac{ac\cdot cov(r_1,r_2)}{\sqrt{a^2D[r_1]\cdot c^2D[r_2]}}=\rho(r_1, r_2)\cdot \frac{a}{\sqrt{a^2}}\cdot \frac{c}{\sqrt{c^2}}=\rho(r_1,r_2)\frac{a}{|a|}\cdot\frac{c}{|c|}=$$
$$=\boxed{\rho(r_1,r_2)\cdot sign(a)\cdot sign(c)}$$
(т.е. при умножении переменных на константы знак корреляции меняется в соответствии со знаками множителей).
4. $p(r_1,r_2)=1\text{ только тогда, когда }r_1=k\cdot r_2+t,\text{ где } k \text{ и } t-\text{константы}$
Далее расчёт ковариации при линейной зависимости:
$$cov(r_1,r_2)=cov(kr_2+t,r_2)=\sum_{i=1}^{n}\sum_{j=1}^{m}(x_i-M[r_1])(y_j - M[r_2])q_{ij}=$$
Учитываем $n=m,~q_{i,j}=0~\forall~i,j:~i\ne j$:
$$=\sum_{i=1}^{n}\sum_{j=1}^{m}(k y_j+t - M[kr_2+t])(y_j-M[r_2])p_{2j}=k\cdot\sum_{j=1}^{m}(y_j-M[r_2])^2 p_{2j}=k\cdot D[r_2]$$
Отсюда:
$$\rho(r_1,r_2)=\rho(kr_2+t,r_2)=\frac{k\cdot D[r_2]}{\sqrt{k^2 D[r_2]\cdot D[r_2]}}=sign(k)$$
Следовательно, чтобы $p(x_1,x_2)=1$, нужно $k>0$ (т.е. положительная линейная зависимость). Если $k<0$, получится $p=-1$.
- В обратную сторону - самостоятельно ($= 1 \Rightarrow$ линейная зависимость)

Рассмотрим график:
![[02 - Areas/Education/Mathematics/Probability Theory & Statistics/Theory/img/005.png]]
Где:
- $h~-~$колебания уровня моря океана
- $S~-~$кол-во проданных пластинок Джастина Бибера

Оценка $S* (h, S)\simeq 0,95$
Надо: $0,95 \pm 0,95$
Все это статическая погрешность.
Статическая связь это необязательно причинно-следственная связь.
### Шкала Чеддока (сила связи)
<table class="table">
  <!-- СТРОКА ЗАГОЛОВКОВ (3 ячейки: пустая + 2 заголовка) -->
  <tr>
    <th></th>
    <th class="c">||ρ(r₁, r₂)||</th>
    <th class="c">Сила связи между $r_1$ и $r_2$</th>
  </tr>
  
  <!-- ПЕРВАЯ СТРОКА ДАННЫХ: вертикальный заголовок (3 строки) -->
  <tr>
    <th class="vertical-header" rowspan="3">предсказать $r_1$ по $r_2$ не особо получится</th>
    <td class="l">0–0.3</td>
    <td class="l">пренебрежимо малая / очень слабая</td>
  </tr>
  
  <!-- ВТОРАЯ СТРОКА ДАННЫХ: автоматически объединена с верхним заголовком -->
  <tr>
    <td class="l">0.3–0.5</td>
    <td class="l">слабая</td>
  </tr>
  
  <!-- ТРЕТЬЯ СТРОКА ДАННЫХ: автоматически объединена с верхним заголовком -->
  <tr>
    <td class="l">0.5–0.7</td>
    <td class="l">средняя</td>
  </tr>
  
  <!-- ЧЕТВЕРТАЯ СТРОКА ДАННЫХ: пустая ячейка слева -->
  <tr>
    <td></td>
    <td class="l">0.7–0.9</td>
    <td class="l">сильная (норм.)</td>
  </tr>
  
  <!-- ПЯТАЯ СТРОКА ДАННЫХ: пустая ячейка слева -->
  <tr>
    <td></td>
    <td class="l">0.9–1.0</td>
    <td class="l">очень сильная</td>
  </tr>
</table>

### Ковариация и матожидание
Связь:
$$m_{11}^0[r_1, r_2] = \sum_{i=1}^n\sum{j=1}{m}(x_i-M[r_1])(y_j-M[r_2])q_{ij}=$$
$$=\underbrace{\sum_{i=1}^n\sum_{j=1}^{m}x_iy_jq_{ij}}_{m_{1,1}[r_1,r_2]} - \underbrace{M[r_1]\sum_{i=1}^n\sum_{j=1}^{m} y_{ij}q_{ij}}_{M[r_2]}-\underbrace{M[r_2]\sum_{i=1}^n\sum_{j=1}^{m}x_iq_{ij}}_{M[r_1]} + M[r_1]M[r_2]\sum_{i=1}^n\sum_{j=1}^{m}q_{ij}$$
$$=\boxed{m_{1,1}[r_1, r_2] - M[r_1]M[r_2]}$$
Сравнимые:$$D[r]=m_2[r]-M^2[r]$$- расщепление с $1\alpha$ на $2\alpha$
$$D[r]=M[(r-M[r])^2]$$
А теперь как же быть с вектором?
Ведь распределение случайной величины можно рассматривать как векторное.
$$M[(\vec{r}-M[\vec{r}])^2]\Rightarrow \sum_{\vec{r}}=M[\overbrace{(\vec{r}-M[\vec{r}])(\vec{r}-M[\vec{r}])}^{\substack{\text{Матричное возведение} \\ \text{в квадрат для векторов}}}]$$
$$\sum_{\vec{r}}=M\left[\left(\begin{pmatrix} r_1 \\ r_2 \end{pmatrix} - \begin{pmatrix} M[r_1] \\ M[r_2] \end{pmatrix}\right)\cdot \left((r_1, r_2)-(M[r_1]\cdot M[r_2])\right)\right]=$$
$$(*)~M[\vec{r}]=M[\begin{pmatrix} r_1 \\ r_2 \end{pmatrix}]=M[\begin{pmatrix} M[r_1] \\ M[r_2] \end{pmatrix}]$$
$$=M[\begin{pmatrix} r_1 - M[r_1] \\ r_2 - M[r_2] \end{pmatrix}((r_1-M[r_1])(r_2-M[r_2]))]=$$
А так как транспонирование матрицы = матрице транспонирования, то:
$$
= M\left[
\begin{pmatrix}
(r_1 - M[r_1])^2 & (r_1 - M[r_1])(r_2 - M[r_2]) \\
(r_2 - M[r_2])(r_1 - M[r_1]) & (r_2 - M[r_2])^2
\end{pmatrix}
\right] =
$$
$$
=\left[
\begin{pmatrix}
M[(r_1 - M[r_1])^2] & M[(r_1 - M[r_1])(r_2 - M[r_2])] \\
M[(r_2 - M[r_2])(r_1 - M[r_1])] & M[(r_2 - M[r_2])^2]
\end{pmatrix}
\right] =
$$
$$
=\begin{pmatrix}
D[r_1] & cov(r_1,r_2) \\
cov(r_2,r_1) & D[r_2]
\end{pmatrix}
$$
Финальные заметки на полях:
> *(cov(x₁,x₂))* — ковариация.
> *(PCA — Principal Component Analysis)* — метод главных компонент, используется для выделения связей между переменными.